# Image Classifier API

This project implements an image classification API using a FastAPI backend and an ONNX model. The model is designed to classify images into one of 1000 ImageNet categories.

## Features

- FastAPI endpoint `/predict` for image classification.
- Uses an ONNX model with built-in preprocessing.
- Dockerized for easy deployment.
- CI pipeline using GitHub Actions to build the Docker image on every push.

## Project Structure

```
.
├── .github/workflows/        # GitHub Actions CI pipeline
│   └── docker-build.yml
├── app.py                    # FastAPI application
├── cerebrium.toml            # Cerebrium deployment configuration
├── convert_to_onnx.py        # Script to convert PyTorch model to ONNX
├── Dockerfile                # Docker configuration for deployment
├── image_classifier.onnx     # The ONNX model file (generated by convert_to_onnx.py)
├── images/                   # Sample images for testing
├── model.py                  # OnnxModel class for loading and running the model
├── pytorch_model.py          # PyTorch model definition (Classifier class)
├── pytorch_model_weights.pth # Pre-trained weights for the PyTorch model (included)
├── README.md                 # This file
├── requirements.txt          # Python dependencies for the runtime environment
├── test.py                   # Unit tests for local model testing
└── test_server.py            # Script to test the deployed Cerebrium endpoint
```

## Setup and Running Locally

### Prerequisites

- Python 3.8+
- pip

### Installation

1.  **Clone the repository:**
    ```bash
    git clone <repository-url>
    cd <repository-directory>
    ```

2.  **Install dependencies:**
    ```bash
    pip install -r requirements.txt
    ```
    *Note: The `requirements.txt` file lists dependencies for running the application. To run the `convert_to_onnx.py` script, you will also need `torch` and `torchvision`. You can install them using: `pip install torch torchvision` (or `pip install torch torchvision --index-url https://download.pytorch.org/whl/cpu` for CPU-only versions if preferred).*

3.  **Generate the ONNX model (if not already present):**
    The `image_classifier.onnx` model is included in the repository. This script uses the `pytorch_model.py` definition and the provided `pytorch_model_weights.pth` (included in this repository) to generate the `image_classifier.onnx` file. If you need to regenerate it (e.g., after changes to `pytorch_model.py` or `convert_to_onnx.py`):
    ```bash
    python convert_to_onnx.py
    ```
    This will create/overwrite `image_classifier.onnx`.

### Running the API Server

Once the setup is complete and `image_classifier.onnx` is present, run the FastAPI application using Uvicorn:

```bash
uvicorn app:app --host 0.0.0.0 --port 8000
```

The API will be accessible at `http://localhost:8000`.

## API Endpoint

### `POST /predict`

Upload an image to this endpoint to get a classification.

-   **Request:** `multipart/form-data` with an `image` field containing the image file.
-   **Response:** JSON object with `class_id` and `probabilities`.

**Example using cURL:**

```bash
curl -X POST -F "image=@images/n01440764_tench.jpg" http://localhost:8000/predict
```

Expected output (probabilities will vary slightly):
```json
{
  "my_result": {
    "class_id": 0,
    "probabilities": [ /* array of 1000 probabilities */ ]
  },
  "status_code": 200
}
```

## Model and Preprocessing

The application uses an ONNX model (`image_classifier.onnx`) for inference.
A key feature of this model is that **image preprocessing steps (resizing, normalization, etc.) are integrated directly into the ONNX graph.**
This means the `OnnxModel` class in `model.py` only needs to load the image, resize it to the expected input dimensions (224x224), and convert it to a float32 NumPy array with pixel values in the 0-255 range. The ONNX model itself handles the rest of the preprocessing internally.

## CI Pipeline (GitHub Actions)

This project includes a Continuous Integration (CI) pipeline defined in `.github/workflows/docker-build.yml`.
The CI pipeline is triggered on every `push` to the repository. Its main job is to:

1.  Check out the latest code.
2.  Build the Docker image using the `Dockerfile` present in the repository.
3.  Tag the Docker image with a unique name based on the current timestamp (e.g., `my-image-name:1678886400`).

This ensures that a buildable Docker image is always available and helps catch integration issues early.

## Deployment (Docker)

The application is designed to be deployed using Docker.

1.  **Build the Docker image:**
    (This step is also performed by the CI pipeline)
    ```bash
    docker build -t image-classifier-app .
    ```

2.  **Run the Docker container:**
    ```bash
    docker run -p 8000:8000 image-classifier-app
    ```
    The API will then be accessible on port 8000 of the Docker host.

## Testing

Unit tests are located in `test.py`. To run them:

```bash
python test.py
```
The tests verify the `OnnxModel`'s prediction capabilities using sample images and also check error handling for invalid inputs.

## Deployment to Cerebrium

This project is configured for deployment on [Cerebrium](https://www.cerebrium.ai/) as a serverless GPU application using a custom Docker image.

1.  **Connect GitHub Repository:** In your Cerebrium dashboard, create a new model and connect it to your forked GitHub repository.
2.  **Configuration:** Cerebrium will use the `Dockerfile` to build the application and `cerebrium.toml` for deployment settings (like hardware, scaling, and entry point). Ensure `cerebrium.toml` is configured to your needs (the provided one sets up a GPU instance and uses `app:app` from `app.py`).
3.  **Deployment:** Once configured, Cerebrium will automatically build the Docker image and deploy the application. Monitor the build and deployment logs on the Cerebrium platform.
4.  **Endpoint URL and API Key:** After successful deployment, Cerebrium will provide you with a unique API endpoint URL for your model and an API key for authentication. You will need these to interact with your deployed model.

## Testing the Deployed Cerebrium Endpoint

Once your model is deployed on Cerebrium and you have your API key and endpoint URL:

1.  **Navigate to the script:** Use the `test_server.py` script to send requests to your live Cerebrium endpoint.

2.  **Run the script:**
    You can test with a specific image:
    ```bash
    python test_server.py --image images/n01440764_tench.jpg --api-key YOUR_CEREBRIUM_API_KEY --endpoint YOUR_CEREBRIUM_ENDPOINT_URL
    ```
    Or run a preset suite of tests:
    ```bash
    python test_server.py --custom-tests --api-key YOUR_CEREBRIUM_API_KEY --endpoint YOUR_CEREBRIUM_ENDPOINT_URL
    ```
    - Replace `YOUR_CEREBRIUM_API_KEY` with the API key provided by Cerebrium.
    - Replace `YOUR_CEREBRIUM_ENDPOINT_URL` with the endpoint URL for your deployed model on Cerebrium.

    The script will print the latency, class ID, and probabilities returned by the deployed model. It also includes a basic health check for the Cerebrium endpoint when running custom tests.
